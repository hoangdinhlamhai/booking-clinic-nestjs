import { Injectable, Logger } from '@nestjs/common';
import { getGroqClient, GROQ_MODEL_STANDARD } from '../agents/models/groq.models';
import {
    TranscriptSegment,
    ProcessedSegment,
    SttProcessingResult,
} from './dto/stt-response.dto';

/**
 * Speech-to-Text Service
 * 
 * Processes audio recordings through a pipeline:
 * 1. Whisper STT (Groq) - Audio ‚Üí Text
 * 2. LLM Role Detection - Identify Doctor vs Patient
 * 3. Medical Text Fixer - Fix pronunciation errors in medical terms
 */
@Injectable()
export class SttService {
    private readonly logger = new Logger(SttService.name);

    /**
     * Call Groq Whisper API to transcribe audio to text
     * 
     * @param audioBlob Audio buffer (WAV, MP3, etc.)
     * @returns Transcription with text and segments
     */
    async transcribeWithGroq(
        audioBlob: Buffer,
    ): Promise<{ text: string; segments: TranscriptSegment[] }> {
        const formData = new FormData();
        // Convert Buffer to Uint8Array for Blob compatibility
        const uint8Array = new Uint8Array(audioBlob);
        const blob = new Blob([uint8Array], { type: 'audio/wav' });
        formData.append('file', blob, 'recording.wav');
        formData.append('model', 'whisper-large-v3');
        formData.append('language', 'vi');
        formData.append('response_format', 'verbose_json');

        const response = await fetch(
            'https://api.groq.com/openai/v1/audio/transcriptions',
            {
                method: 'POST',
                headers: { Authorization: `Bearer ${process.env.GROQ_API_KEY}` },
                body: formData,
            },
        );

        if (!response.ok) {
            throw new Error(`Groq API error: ${response.statusText}`);
        }

        const data = await response.json();

        return {
            text: data.text || '',
            segments: data.segments || [],
        };
    }

    /**
     * Prepare transcription segments for LLM role detection
     */
    prepareSegmentsForRoleDetection(transcription: {
        text: string;
        segments: TranscriptSegment[];
    }): { role: string; raw_text: string; start: number; end: number }[] {
        if (transcription.segments.length > 0) {
            return transcription.segments.map((seg) => ({
                role: 'Ng∆∞·ªùi n√≥i', // Placeholder - LLM will determine actual role
                raw_text: seg.text,
                start: seg.start,
                end: seg.end,
            }));
        }

        // Fallback if no segments
        if (transcription.text) {
            return [
                {
                    role: 'Ng∆∞·ªùi n√≥i',
                    raw_text: transcription.text,
                    start: 0,
                    end: 0,
                },
            ];
        }

        return [];
    }

    /**
     * Use LLM to analyze content and detect speaker roles
     * Based on conversation context to determine who is Doctor vs Patient
     */
    async detectSpeakerRoleByContent(
        segments: { role: string; raw_text: string; start: number; end: number }[],
    ): Promise<
        { role: string; raw_text: string; start: number; end: number }[]
    > {
        if (segments.length === 0) return segments;

        // Create prompt with all segments
        const conversationText = segments
            .map((seg, i) => `[${i}] "${seg.raw_text.trim()}"`)
            .join('\n');

        const prompt = `B·∫°n l√† chuy√™n gia ph√¢n t√≠ch h·ªôi tho·∫°i y khoa ti·∫øng Vi·ªát.
D∆∞·ªõi ƒë√¢y l√† transcript cu·ªôc kh√°m b·ªánh. H√£y x√°c ƒë·ªãnh vai tr√≤ ng∆∞·ªùi n√≥i cho t·ª´ng ƒëo·∫°n.

QUY T·∫ÆC X√ÅC ƒê·ªäNH VAI TR√í:
- B√ÅC Sƒ®: H·ªèi tri·ªáu ch·ª©ng, h·ªèi b·ªánh s·ª≠, ƒë∆∞a ra ch·∫©n ƒëo√°n, k√™ ƒë∆°n thu·ªëc, h∆∞·ªõng d·∫´n ƒëi·ªÅu tr·ªã
- B·ªÜNH NH√ÇN: M√¥ t·∫£ tri·ªáu ch·ª©ng ("t√¥i b·ªã...", "t√¥i th·∫•y..."), x∆∞ng "ch√†o b√°c sƒ©", tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ b·∫£n th√¢n

MANH M·ªêI QUAN TR·ªåNG:
- Ai n√≥i "Ch√†o b√°c sƒ©" ‚Üí B·ªÜNH NH√ÇN
- Ai h·ªèi "b·∫°n/anh/ch·ªã c√≥ tri·ªáu ch·ª©ng g√¨?" ‚Üí B√ÅC Sƒ®  
- Ai m√¥ t·∫£ "t√¥i ƒëau...", "t√¥i b·ªã..." ‚Üí B·ªÜNH NH√ÇN
- Ai h·ªèi "c√≥ s·ªët kh√¥ng?", "u·ªëng thu·ªëc g√¨ ch∆∞a?" ‚Üí B√ÅC Sƒ®

H·ªòI THO·∫†I:
${conversationText}

Tr·∫£ v·ªÅ CH√çNH X√ÅC ƒë·ªãnh d·∫°ng JSON array sau, KH√îNG c√≥ text kh√°c:
[{"index": 0, "role": "B√°c sƒ©"}, {"index": 1, "role": "B·ªánh nh√¢n"}, ...]`;

        try {
            this.logger.debug('üîç Analyzing speaker roles with Groq...');

            const groq = getGroqClient();
            const completion = await groq.chat.completions.create({
                messages: [{ role: 'user', content: prompt }],
                model: GROQ_MODEL_STANDARD,
                temperature: 0.1,
            });

            const responseText = completion.choices[0]?.message?.content || '';

            // Extract JSON
            const jsonMatch = responseText.match(/\[[\s\S]*\]/);
            if (!jsonMatch) {
                this.logger.warn('LLM did not return valid JSON, keeping original roles');
                return segments;
            }

            const roleAssignments: { index: number; role: string }[] = JSON.parse(
                jsonMatch[0],
            );

            // Update segments with new roles from LLM
            const updatedSegments = segments.map((seg, i) => {
                const assignment = roleAssignments.find((r) => r.index === i);
                if (assignment) {
                    return { ...seg, role: assignment.role };
                }
                return seg;
            });

            this.logger.debug('‚úÖ LLM role detection completed');
            return updatedSegments;
        } catch (error) {
            this.logger.error('‚ùå LLM role detection error:', error);
            // Fallback: keep original roles
            return segments;
        }
    }

    /**
     * Use LLM to fix medical terminology errors
     * ONLY fixes typos, does NOT add new content
     */
    async fixMedicalText(text: string): Promise<string> {
        if (!text || text.trim().length === 0) return text;

        try {
            const groq = getGroqClient();
            const completion = await groq.chat.completions.create({
                messages: [
                    {
                        role: 'system',
                        content: `B·∫°n l√† chuy√™n gia hi·ªáu ch·ªânh vƒÉn b·∫£n y khoa ti·∫øng Vi·ªát.
NHI·ªÜM V·ª§: Ch·ªâ s·ª≠a l·ªói ch√≠nh t·∫£ v√† ph√°t √¢m sai trong ƒëo·∫°n vƒÉn ƒë∆∞·ª£c chuy·ªÉn t·ª´ gi·ªçng n√≥i.

QUY T·∫ÆC B·∫ÆT BU·ªòC:
1. TUY·ªÜT ƒê·ªêI KH√îNG th√™m n·ªôi dung m·ªõi
2. TUY·ªÜT ƒê·ªêI KH√îNG x√≥a b·ªõt n·ªôi dung
3. TUY·ªÜT ƒê·ªêI KH√îNG vi·∫øt l·∫°i c√¢u
4. Ch·ªâ s·ª≠a l·ªói ph√°t √¢m th∆∞·ªùng g·∫∑p:
   - "ƒëau th∆∞·ª£ng v·ªãt" ‚Üí "ƒëau th∆∞·ª£ng v·ªã"
   - "b·ªã s·ª•p" ‚Üí "b·ªã s·ªët"  
   - "ƒÉn ch√≠ch" ‚Üí "ƒÉn ki√™ng"
   - "ti√™u chu·∫©n" ‚Üí "tri·ªáu ch·ª©ng"
5. Gi·ªØ nguy√™n s·ªë t·ª´ v√† √Ω nghƒ©a g·ªëc
6. Tr·∫£ v·ªÅ CH√çNH X√ÅC ƒëo·∫°n vƒÉn g·ªëc v·ªõi l·ªói ƒë√£ s·ª≠a, KH√îNG tr·∫£ l·ªùi hay gi·∫£i th√≠ch th√™m.`,
                    },
                    { role: 'user', content: text },
                ],
                model: GROQ_MODEL_STANDARD,
                temperature: 0.05,
            });

            // Add artificial delay to respect rate limits if calling in loop
            await new Promise((resolve) => setTimeout(resolve, 200));

            return completion.choices[0]?.message?.content || text;
        } catch (error) {
            this.logger.error('‚ùå Medical fixer error:', error);
            return text;
        }
    }

    /**
     * Main processing method - orchestrates entire STT pipeline
     * Flow: Whisper STT ‚Üí LLM Role Detection ‚Üí Medical Text Fixer
     * 
     * @param audioBuffer Audio file buffer
     * @returns Processed segments with roles and cleaned text
     */
    async processAudioFile(audioBuffer: Buffer): Promise<SttProcessingResult> {
        try {
            this.logger.log(`üé§ Received audio: ${audioBuffer.length} bytes`);

            // Step 1: Whisper STT - Convert audio to text
            this.logger.log('üîä Running Whisper STT...');
            const transcription = await this.transcribeWithGroq(audioBuffer);
            this.logger.log(`üìù Transcription: ${transcription.text.substring(0, 100)}...`);
            this.logger.debug(`üìä Segments count: ${transcription.segments.length}`);

            // If no text, return empty
            if (!transcription.text || transcription.text.trim().length === 0) {
                return {
                    success: true,
                    segments: [],
                    raw_text: '',
                    num_speakers: 0,
                };
            }

            // Step 2: Prepare segments for role detection
            const preparedSegments =
                this.prepareSegmentsForRoleDetection(transcription);
            this.logger.debug(`‚úÖ Prepared segments: ${preparedSegments.length}`);

            // Step 3: LLM Role Detection - Analyze content to determine Doctor/Patient
            const segmentsWithRoles =
                await this.detectSpeakerRoleByContent(preparedSegments);

            // Step 4: Medical Text Fixer - Fix medical terminology errors
            this.logger.log('ü©∫ Running Medical Text Fixer...');
            const processedSegments: ProcessedSegment[] = [];
            for (const seg of segmentsWithRoles) {
                const clean_text = await this.fixMedicalText(seg.raw_text);
                processedSegments.push({
                    ...seg,
                    clean_text,
                });
            }

            this.logger.log('‚úÖ STT Processing complete!');

            return {
                success: true,
                segments: processedSegments,
                raw_text: transcription.text,
                num_speakers: 2, // Assumed 2 speakers (Doctor + Patient)
            };
        } catch (error) {
            this.logger.error('‚ùå STT Processing error:', error);
            throw error;
        }
    }
}
